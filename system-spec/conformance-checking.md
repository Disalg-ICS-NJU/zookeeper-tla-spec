# Conformance Checking

With the [Model Checking-driven Explorative Testing (MET)](https://github.com/Lingzhi-Ouyang/MET) framework, we can perform conformance checking in a low-cost way. 

Based on the instrumented ZooKeeper system, the test execution environment can intercept and release target events in designated orders. We can explore whether the traces generated by TLC can be reproduced in the manipulated system or not. Once the validation fails, there might exist some inconsistency between the written specification and the implemented code. Then, the specification should be corrected to further match the code with human efforts. After more and more rounds of conformance checking, the system specification can be of higher accuracy. 



We have executed conformance checking after completing system specification. We discover several divergences between specification and code. 

For example, the very first version of system specification assumes whenever the action *LeaderProcessRequest* is scheduled (simulating the logic of a leader processing a SetData request), the client session has already been established successfully. The main purpose of this design is to mitigate state explosion problem. 

```TLA+
LeaderProcessRequest(i) == 
        \** Enabling conditions
        /\ CheckTransactionNum 
        /\ IsON(i)
        /\ IsLeader(i)
        /\ zabState[i] = BROADCAST
        \** Do something
        ... 
```

However, the client session creation is also regarded as a transaction in ZooKeeper, and should be committed by a quorum of servers. If not enough servers are ready to commit this transaction, the client session cannot be established successfully, and the following write request will not be processed. We find this divergence during conformance checking and fix it by adding extra enabling restrictions in the action *LeaderProcessRequest*.  (See *[ZkV3_7_0.tla](zk-3.7/ZkV3_7_0.tla)*)

```TLA+
LeaderProcessRequest(i) == 
        \** Enabling conditions
        /\ CheckTransactionNum 
        /\ IsON(i)
        /\ IsLeader(i)
        /\ zabState[i] = BROADCAST
        \** The following condition is added to ensure that quorums of servers are ready to process transactions
        /\ LET inBroadcast == {s \in forwarding[i]: zabState[s] = BROADCAST}
           IN IsQuorum(inBroadcast)
        \** Do something
        ...
```



Another example is about the modeling of the environment failures. In the protocol specification of Zab, we use the actions *Timeout* and *Restart* to simulate the internal effects of network partitions and node crashes and restarts. In this way, state space can be compressed a lot without losing the ability of triggering bugs (More detailed explanations can be found at the comments before the action definition *Timeout* and *Restart* in *[Zab.tla](../protocol-spec/Zab.tla)*). 

```
\/ \E i, j \in Server: Timeout(i, j)
\/ \E i \in Server:    Restart(i)
```

In order to ease the conformance checking and check whether the specified effects of failures match the ones in the real world, we refine the failures to *NodeCrash*, *NodeStart*, *PartitionStart* and *PartitionRecover* (See *[ZkV3_7_0.tla](zk-3.7/ZkV3_7_0.tla)*). These failure events in our system specification are more fundamental and can generate the failures we model in the protocol specification. 

    \/ \E i, j \in Server: PartitionStart(i, j)
    \/ \E i, j \in Server: PartitionRecover(i, j)
    \/ \E i \in Server:    NodeCrash(i)
    \/ \E i \in Server:    NodeStart(i)



Through conformance checking, the system specification has been perfected a lot and got approved by the ZooKeeper committee. Both protocol specification and system specification are under review and about to be merged in the ZooKeeper repository.
